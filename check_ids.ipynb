{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.model_zoo.arcface_onnx import ArcFaceONNX\n",
    "from insightface.utils import face_align\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(\"\", \"..\")))\n",
    "sys.path.append(\"/home/ubuntu/AutoLoRADiscovery/discover_lora_diffusion/weights2weights/\")\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from discover_lora_diffusion.weights2weights.lora_w2w import LoRAw2w\n",
    "from discover_lora_diffusion.weights2weights.utils import unflatten\n",
    "from diffusers import DiffusionPipeline \n",
    "from peft import PeftModel\n",
    "from peft.utils.save_and_load import load_peft_weights\n",
    "# \n",
    "\n",
    "# import clip\n",
    "import wandb\n",
    "# import clip\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=\"cuda\")\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "preprocess = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "dino_model = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "dino_model = dino_model.cuda()\n",
    "\n",
    "\n",
    "def rtn_face_get(self, img, face):\n",
    "    aimg = face_align.norm_crop(img, landmark=face.kps, image_size=self.input_size[0])\n",
    "    #print(cv2.imwrite(\"aimg.png\", aimg))\n",
    "    face.embedding = self.get_feat(aimg).flatten()\n",
    "    face.crop_face = aimg\n",
    "    return face.embedding\n",
    "\n",
    "ArcFaceONNX.get = rtn_face_get\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(256, 256))\n",
    "\n",
    "def crop_face(img, face_ratio=1.0):\n",
    "    cv2_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    faces = app.get(cv2_img)\n",
    "    try:\n",
    "        bbox = faces[0]['bbox']\n",
    "\n",
    "        # get largest possible square around center point\n",
    "        w,h = img.width, img.height\n",
    "        box_w = bbox[2] - bbox[0]\n",
    "        box_h = bbox[3] - bbox[1]\n",
    "        if box_w < box_h:\n",
    "            diff = box_h - box_w\n",
    "            bbox[0] -= diff // 2\n",
    "            bbox[2] += diff // 2\n",
    "        else:\n",
    "            diff = box_w - box_h\n",
    "            bbox[1] -= diff // 2\n",
    "            bbox[3] += diff // 2\n",
    "        dist_to_left = bbox[0]\n",
    "        dist_to_right = w - bbox[2]\n",
    "        dist_to_top = bbox[1]\n",
    "        dist_to_bottom = h - bbox[3]\n",
    "        min_dist = min(dist_to_left, dist_to_right, dist_to_top, dist_to_bottom)\n",
    "        min_dist = int(min_dist * face_ratio) - 1\n",
    "        bbox = [bbox[0]-min_dist, bbox[1]-min_dist, bbox[2]+min_dist, bbox[3]+min_dist]\n",
    "        face_image = img.crop(bbox)\n",
    "\n",
    "        return face_image, faces[0].embedding\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "idens = torch.load(\"/mnt/rd/identity_df.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idens.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(emb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(_emb_norm)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_emb_norm = emb / np.linalg.norm(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show me face of /mnt/rd/img_align_celeba/000005.jpg\n",
    "import requests\n",
    "img1 = Image.open(\"/home/ubuntu/AutoLoRADiscovery/junk/1691527680903.jpeg\")\n",
    "img2 = Image.open(requests.get(\"https://foivospar-arc2face.hf.space/file=/tmp/gradio/f9e31998958141179c8c265d3b91ba8892014605/image.png\", stream=True).raw)\n",
    "face1, emb1 = crop_face(img1)\n",
    "face2, emb2 = crop_face(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face1.show(), face2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "cos_sim(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show me face of /mnt/rd/img_align_celeba/000005.jpg\n",
    "img = Image.open(\"/mnt/rd/img_align_celeba/000005.jpg\")\n",
    "face, emb = crop_face(img)\n",
    "face.show(), img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/AutoLoRADiscovery/discover_lora_diffusion/weights2weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "weights = torch.load(\"/mnt/rd/all_weights.pt\", map_location=\"cpu\")#.bfloat16().cuda()#.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.load(\"/mnt/rd/V.pt\", map_location=\"cuda\")#.half()\n",
    "mean = torch.load(\"/mnt/rd/mean.pt\", map_location=\"cuda\")#.half()\n",
    "std = torch.load(\"/mnt/rd/std.pt\", map_location=\"cuda\")#.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = weights - mean.bfloat16()\n",
    "    x = x / std.bfloat16()\n",
    "    actual_pcas = torch.matmul(x, V.bfloat16())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.max(), weights.min(), weights.mean(), weights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discover_lora_vae.models import LoraVAE, config\n",
    "import torch\n",
    "\n",
    "lora_vae = LoraVAE(\n",
    "    input_dim=99_648,\n",
    "    latent_dim=4096,\n",
    ").cuda()\n",
    "\n",
    "lora_vae.load_state_dict(torch.load(\"/mnt/rd/model-out-2/checkpoint-48000\", map_location=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "test_weight = weights#.float()\n",
    "batch_size = 4096\n",
    "with torch.no_grad():\n",
    "    test_pca = torch.zeros(test_weight.shape[0], 4096, device=\"cuda\")\n",
    "    # reconstructed = torch.zeros(test_weight.shape[0], 99_648, device=\"cpu\")\n",
    "    for i in tqdm.tqdm(range(0, test_weight.shape[0], batch_size)):\n",
    "        merged = lora_vae.encode( \n",
    "            lora_vae.apply_std_on_weights(test_weight[i:i+batch_size].float().cuda()),\n",
    "            # mean_logvar=True#\n",
    "              )[0]#.chunk(2, dim=-1)[0]\n",
    "        # decoded_lora = lora_vae.deapply_std_on_weights(lora_vae.decode(merged))\n",
    "\n",
    "        # merged = torch.cat([mean, logvar], dim=-1)\n",
    "        test_pca[i:i+batch_size] = merged\n",
    "        # reconstructed[i:i+batch_size] = decoded_lora\n",
    "    # test_pca = ((test_weight - mean) / (std)) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lora_vae.apply_std_on_weights(weights[0:1]).cpu().numpy()[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0].min(), weights[0].max(), weights[0].mean(), weights[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(weights[0].cpu().numpy()/0.0152)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(weights[0].cpu().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming your latent vectors are in a numpy array called 'latent_vectors'\n",
    "# with shape (100, 4096)\n",
    "\n",
    "# 1. Perform PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(test_pca[:10000, :].cpu().numpy())\n",
    "\n",
    "# 2. Calculate cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# 3. Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('PCA: Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot explained variance ratio for first 50 components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(50), pca.explained_variance_ratio_[:50])\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA: Explained Variance Ratio of First 50 Components')\n",
    "plt.show()\n",
    "\n",
    "# 5. Scatter plot of first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA: First Two Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# 6. Print the number of components needed to explain 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"Number of components needed to explain 95% of variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is in a numpy array called 'latent_vectors'\n",
    "# with shape (10000, 4096)\n",
    "\n",
    "# Create a UMAP model\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "embedding = reducer.fit_transform(test_pca[:1000, :].cpu().numpy())\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "plt.title('UMAP projection of the latent space')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is in a numpy array called 'latent_vectors'\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "latent_2d = tsne.fit_transform(test_pca[:1000, :].cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1])\n",
    "plt.title('t-SNE visualization of latent space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming your latent vectors are in a numpy array called 'latent_vectors'\n",
    "# with shape (100, 4096)\n",
    "\n",
    "# 1. Perform PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(test_pca[:, :].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from discover_lora_diffusion.models import LoraDiffusion\n",
    "import diffusers\n",
    "\n",
    "config = {\n",
    "        \"_class_name\": \"UnCLIPScheduler\",\n",
    "        \"_diffusers_version\": \"0.17.0.dev0\",\n",
    "        \"clip_sample\": True,\n",
    "        \"clip_sample_range\": 10.0,  # Adjusted to match your data range\n",
    "        \"num_train_timesteps\": 1000,  # Reduced from 1000\n",
    "        \"prediction_type\": \"sample\",\n",
    "        \"variance_type\": \"fixed_small_log\"\n",
    "}\n",
    "\n",
    "# scheduler = diffusers.UnCLIPScheduler.from_config(\"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"scheduler\")\n",
    "scheduler = diffusers.UnCLIPScheduler.from_config(config)\n",
    "\n",
    "# scheduler.set_timesteps(50)\n",
    "lora_diffusion = LoraDiffusion(\n",
    "        data_dim=99_648,\n",
    "        model_dim=768,\n",
    "        ff_mult=3,\n",
    "        chunks=1,\n",
    "        act=torch.nn.SiLU,\n",
    "        num_blocks=6,\n",
    "        layers_per_block=4,\n",
    "    ).cuda()\n",
    "lora_diffusion.load_state_dict(torch.load(\"/mnt/rd/diff_lora_mdl/checkpoint-clpface_newdataset22_small_50step_denoise_reluflat-134000\", map_location=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "import copy\n",
    "from discover_lora_diffusion.weights2weights.lora_w2w import LoRAw2w\n",
    "from discover_lora_diffusion.weights2weights.utils import unflatten\n",
    "from diffusers import DiffusionPipeline , logging\n",
    "from peft import PeftModel\n",
    "from peft.utils.save_and_load import load_peft_weights\n",
    "import torch\n",
    "weight_dimensions = torch.load(\"/mnt/rd/weight_dimensions.pt\")\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v51\", \n",
    "                                            torch_dtype=torch.float16,safety_checker = None,\n",
    "                                            requires_safety_checker = False).to(device)\n",
    "pipe_unet = pipe.unet                               \n",
    "def generate_from_weights(weight):\n",
    "    if os.path.exists(\"/mnt/rd/inference_lora_2\"):\n",
    "        os.system(\"rm -rf /mnt/rd/inference_lora_2\")\n",
    "\n",
    "    unflatten(weight.unsqueeze(0).detach().clone(), weight_dimensions, \"/mnt/rd/inference_lora_2\")\n",
    "    logging.disable_progress_bar()\n",
    "    \n",
    "\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    pipe.unet = copy.deepcopy(pipe_unet)\n",
    "    pipe.unet = PeftModel.from_pretrained(pipe.unet, \"/mnt/rd/inference_lora_2/unet\", adapter_name=\"identity1\")\n",
    "    adapters_weights1 = load_peft_weights(\"/mnt/rd/inference_lora_2/unet\", device=\"cuda:0\")\n",
    "    pipe.unet.load_state_dict(adapters_weights1, strict = False)\n",
    "    pipe.unet.to(\"cuda\", torch.float16)\n",
    "\n",
    "    # torch rng \n",
    "    generator = torch.Generator(device=\"cuda\")\n",
    "    generator.manual_seed(0)\n",
    "    negative_prompt = \"low quality, blurry, unfinished, cartoon\"\n",
    "    images = pipe([\"sks person with dog\", \"sks person with dog\"],\n",
    "                   negative_prompt=[negative_prompt]*2,\n",
    "    \n",
    "     height=640, width=640, num_inference_steps=50, guidance_scale=2.5, generator=generator).images\n",
    "    canvas = Image.new('RGB', (640, 645*2))\n",
    "    for i, image in enumerate(images):\n",
    "        canvas.paste(image, (0, 650*i))\n",
    "\n",
    "    # return images[0]\n",
    "    return canvas   \n",
    "    # save as low res image\n",
    "    imgbyte = io.BytesIO()\n",
    "    canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "    imgbyte.seek(0)\n",
    "\n",
    "\n",
    "\n",
    "    display(IPImage(data=imgbyte.read(), width=640*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_weights = weights[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_weight = lora_vae.deapply_std_on_weights(lora_vae.decode(lora_vae.encode(lora_vae.apply_std_on_weights(_weights.cuda()))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_weight.shape\n",
    "show_the_lats(_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = torch.randn(1, 99_648).cuda()\n",
    "# _latents = scheduler.add_noise(lora_vae.apply_std_on_weights(weights[0:1]).cuda(), noisy, torch.Tensor([20]).type(torch.int).cuda())#torch.randn(1, 99_648).cuda()  * scheduler.init_noise_sigma \n",
    "latents = noisy\n",
    "# Create an unconditioned embedding for CFG\n",
    "# uncond_face_emb = torch.zeros_like(face_emb1)\n",
    "def show_the_lats(latents, return_img=False):\n",
    "    canvas = generate_from_weights(latents[0].detach().clone())\n",
    "    if return_img:\n",
    "        return canvas\n",
    "\n",
    "    import io\n",
    "    from IPython.display import display, Image as IPImage\n",
    "    imgbyte = io.BytesIO()\n",
    "    canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "    imgbyte.seek(0)\n",
    "\n",
    "\n",
    "    display(IPImage(data=imgbyte.read(), width=canvas.width))\n",
    "# prev_latents = [latents]\n",
    "# latents, _ = lora_diffusion(face_emb1)\n",
    "mid_render = []\n",
    "with torch.no_grad():\n",
    "    for t in scheduler.timesteps:\n",
    "        # latents = scheduler.scale_model_input(latents, t)\n",
    "        noise_pred, pred_cond = lora_diffusion(\n",
    "            latents, \n",
    "            t=t.unsqueeze(0).cuda().half(), \n",
    "            face_embeddings=torch.Tensor(idens.iloc[1].values[:-1]).unsqueeze(0).cuda()\n",
    "        )\n",
    "        \n",
    "        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "        if ((t % 3 == 0) and (t < 30)) or ((t == (len(scheduler.timesteps) - 1))):\n",
    "            print(t)\n",
    "            img = show_the_lats(lora_vae.deapply_std_on_weights(latents), return_img=True)\n",
    "            mid_render.append(img)\n",
    "latents = lora_vae.deapply_std_on_weights(latents)\n",
    "\n",
    "\n",
    "\n",
    "show_the_lats(latents)\n",
    "\n",
    "canvas = Image.new('RGB', (mid_render[0].width*len(mid_render), mid_render[0].height))\n",
    "for i, image in enumerate(mid_render):\n",
    "    canvas.paste(image, (mid_render[0].width*i, 0))\n",
    "\n",
    "import io\n",
    "from IPython.display import display, Image as IPImage\n",
    "imgbyte = io.BytesIO()\n",
    "canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "imgbyte.seek(0)\n",
    "\n",
    "\n",
    "display(IPImage(data=imgbyte.read(), width=canvas.width))\n",
    "\n",
    "# show_the_lats(weights[0:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idens.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "import copy\n",
    "from discover_lora_diffusion.weights2weights.lora_w2w import LoRAw2w\n",
    "from discover_lora_diffusion.weights2weights.utils import unflatten\n",
    "from diffusers import DiffusionPipeline , logging\n",
    "from peft import PeftModel\n",
    "\n",
    "from peft.utils.save_and_load import load_peft_weights\n",
    "import torch\n",
    "weight_dimensions = torch.load(\"/mnt/rd/weight_dimensions.pt\")\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v51\", \n",
    "                                            torch_dtype=torch.float16,safety_checker = None,\n",
    "                                            requires_safety_checker = False).to(device)\n",
    "pipe_unet = pipe.unet                               \n",
    "def generate_from_weights(weight):\n",
    "    if os.path.exists(\"/mnt/rd/inference_lora\"):\n",
    "        os.system(\"rm -rf /mnt/rd/inference_lora\")\n",
    "\n",
    "    unflatten(weight.unsqueeze(0).detach().clone(), weight_dimensions, \"/mnt/rd/inference_lora\")\n",
    "    logging.disable_progress_bar()\n",
    "    \n",
    "\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    pipe.unet = copy.deepcopy(pipe_unet)\n",
    "    pipe.unet = PeftModel.from_pretrained(pipe.unet, \"/mnt/rd/inference_lora/unet\", adapter_name=\"identity1\")\n",
    "    adapters_weights1 = load_peft_weights(\"/mnt/rd/inference_lora/unet\", device=\"cuda:0\")\n",
    "    pipe.unet.load_state_dict(adapters_weights1, strict = False)\n",
    "    pipe.unet.to(\"cuda\", torch.float16)\n",
    "\n",
    "    # torch rng \n",
    "    generator = torch.Generator(device=\"cuda\")\n",
    "    # generator.manual_seed(0)\n",
    "    negative_prompt = \"low quality, blurry, unfinished, cartoon\"\n",
    "    images = pipe([\"sks person\"]*2,\n",
    "                  negative_prompt=[negative_prompt]*2,\n",
    "    \n",
    "     height=640, width=640, num_inference_steps=50, guidance_scale=2.5, generator=generator).images\n",
    "    canvas = Image.new('RGB', (640*2, 640*2))\n",
    "    for i, image in enumerate(images):\n",
    "        canvas.paste(image, (640*i, 320))\n",
    "\n",
    "    # return canvas\n",
    "\n",
    "    # save as low res image\n",
    "    imgbyte = io.BytesIO()\n",
    "    canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "    imgbyte.seek(0)\n",
    "\n",
    "    display(IPImage(data=imgbyte.read(), width=640*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_latent(latent):\n",
    "    # convert latent to weights\n",
    "    weights = lora_vae.deapply_std_on_weights(lora_vae.decode(latent.float().unsqueeze(0).cuda())).squeeze(0)\n",
    "    return generate_from_weights(torch.tensor(weights, device=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent = torch.randn(4096).cuda()\n",
    "generate_from_latent(random_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming test_pca is your tensor of latent vectors\n",
    "with torch.no_grad():\n",
    "    latent_np = torch.Tensor(test_pca)[:, :].float().cuda()#.cpu().numpy()\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov_matrix = torch.cov(latent_np.T)\n",
    "\n",
    "    # Perform eigendecomposition\n",
    "    eigenvalues, eigenvectors = torch.linalg.eig(cov_matrix)\n",
    "    eigenvalues = eigenvalues.cpu().numpy()\n",
    "    eigenvectors = eigenvectors.cpu().numpy()\n",
    "\n",
    "    # Sort eigenvectors by eigenvalues in descending order\n",
    "    idx = eigenvalues.argsort()[::-1]   \n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "def manipulate_latent(latent, eigenvector, magnitude):\n",
    "    return latent + torch.tensor(eigenvector * magnitude, device=latent.device)\n",
    "\n",
    "\n",
    "\n",
    "# Display interpolation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_pca(pc):\n",
    "    inverted = pca.inverse_transform(pc.cpu().numpy())\n",
    "    inverted = torch.tensor(inverted, device=\"cuda\")\n",
    "    return generate_from_latent(inverted)\n",
    "    # with torch.no_grad():\n",
    "        \n",
    "    #     weights = pc.bfloat16() @ V.T.bfloat16()\n",
    "    #     weights = weights * mean.bfloat16()\n",
    "    #     weights = weights + mean.bfloat16()\n",
    "    #     generate_from_weights(torch.tensor(weights, device=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors[:,0].min(), eigenvectors[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate images\n",
    "from tqdm import trange\n",
    "base_latent = test_pca[0]\n",
    "# original_image = generate_from_latent(base_latent)\n",
    "# manipulated_image = generate_from_latent(manipulated_latent)\n",
    "\n",
    "\n",
    "# Interpolation along an eigenvector\n",
    "def interpolate_eigenvector(base_latent, eigenvector, start=-10, end=10, steps=3):\n",
    "    for magnitude in np.linspace(start, end, steps):\n",
    "        yield manipulate_latent(base_latent, eigenvector, magnitude), (magnitude)\n",
    "\n",
    "# Generate interpolation\n",
    "pbar = tqdm.tqdm(total=3*3*eigenvectors.shape[1])\n",
    "for ei in range(eigenvectors.shape[1]):\n",
    "    rows = []\n",
    "    for idi in range(3):\n",
    "        base_latent = torch.Tensor(test_pca[idi])\n",
    "        interpolation = list(interpolate_eigenvector(base_latent, eigenvectors[:, ei]))\n",
    "        images = []\n",
    "        for i, (latent, magnitude) in enumerate(interpolation):\n",
    "            # print (f\"Interpolation {i+1}, magnitude: {magnitude}\")\n",
    "            image = generate_from_latent(latent)\n",
    "            images.append(image)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        canvas = Image.new('RGB', (images[0].width*len(images), images[0].height))\n",
    "        for i, image in enumerate(images):\n",
    "            canvas.paste(image, (images[0].width*i, 0))\n",
    "        \n",
    "        rows.append(canvas)\n",
    "\n",
    "    canvas = Image.new('RGB', (rows[0].width, rows[0].height*len(rows)))\n",
    "    for i, row in enumerate(rows):\n",
    "        canvas.paste(row, (0, row.height*i))\n",
    "    imgbyte = io.BytesIO()\n",
    "    canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "    canvas.save(\"/home/ubuntu/AutoLoRADiscovery/out/{}.jpg\".format(ei), format=\"JPEG\", quality=20)\n",
    "    imgbyte.seek(0)\n",
    "\n",
    "\n",
    "    display(IPImage(data=imgbyte.read(), width=canvas.width))\n",
    "# interpolation = list(interpolate_eigenvector(base_latent, eigenvectors[:, 2]))\n",
    "# for i, (latent, magnitude) in enumerate(interpolation):\n",
    "#     print (f\"Interpolation {i+1}, magnitude: {magnitude}\")\n",
    "#     generate_from_latent(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate images\n",
    "base_latent = test_pca[0]\n",
    "original_image = generate_from_latent(base_latent)\n",
    "# manipulated_image = generate_from_latent(manipulated_latent)\n",
    "\n",
    "\n",
    "# Interpolation along an eigenvector\n",
    "def interpolate_eigenvector(base_latent, eigenvector, start=-2, end=2, steps=10):\n",
    "    for magnitude in np.linspace(start, end, steps):\n",
    "        yield manipulate_latent(base_latent, eigenvector, magnitude), (magnitude)\n",
    "\n",
    "# Generate interpolation\n",
    "interpolation = list(interpolate_eigenvector(base_latent, eigenvectors[:, 0]))\n",
    "for i, (latent, magnitude) in enumerate(interpolation):\n",
    "    print (f\"Interpolation {i+1}, magnitude: {magnitude}\")\n",
    "    generate_from_latent(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_pca = torch.Tensor(latent_reconstructed[0]) #/ 2\n",
    "decoded_lora = lora_vae.deapply_std_on_weights(lora_vae.decode(blended_pca.unsqueeze(0).cuda())).squeeze(0)\n",
    "generate_from_weights(decoded_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_pca = (test_pca[0] + ((test_pca[1] * 9))) / 10\n",
    "decoded_lora = lora_vae.deapply_std_on_weights(lora_vae.decode(blended_pca.unsqueeze(0).cuda())).squeeze(0)\n",
    "generate_from_weights(decoded_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "idens = torch.load(\"/mnt/rd/identity_df.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for folder in list(idens.index): \n",
    "    labels.append(idens.loc[folder][label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(idens.iloc[0].values[:-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weights = torch.load(\"/mnt/rd/all_weights.pt\", map_location=\"cuda\")#.half()\n",
    "V = torch.load(\"/mnt/rd/V.pt\", map_location=\"cuda\")#.half()\n",
    "std = torch.load(\"/mnt/rd/std.pt\", map_location=\"cuda\")#.half()\n",
    "std_w2w = torch.load(\"/mnt/rd/std_w2w.pt\", map_location=\"cuda\") * 5 #.half() \n",
    "mean = torch.load(\"/mnt/rd/mean.pt\", map_location=\"cuda\")#.half()\n",
    "clip_embs = torch.load(\"/home/ubuntu/AutoLoRADiscovery/celeba_arc_embs.pt\", map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_std = test_pca.std(dim=0)\n",
    "latent_mean = test_pca.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save std and mean to ./latent_properties.pt\n",
    "torch.save((latent_std,latent_mean), \"./latent_properties.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_latent = (test_pca - latent_mean) / latent_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_latent.min(), normalized_latent.max(), normalized_latent.mean(), normalized_latent.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pca.min(), test_pca.max(), test_pca.mean(), test_pca.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# plt.plot((weights[0].unsqueeze(0)).squeeze(0).cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.plot((apply_std_on_weights(weights[0].unsqueeze(0))).squeeze(0).cpu().numpy())\n",
    "# # plt.show()\n",
    "# plt.plot(((weights[0]).unsqueeze(0)).squeeze(0).cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.plot(((reconstructed[0]).unsqueeze(0)).squeeze(0).cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "# # # plt.plot(std_w2w.cpu().numpy())\n",
    "# # # plt.show()\n",
    "\n",
    "# # # plt.plot(std_w2w.abs().log().cpu().numpy()/10.0)\n",
    "# # # plt.show()\n",
    "# plt.plot(test_pca[1].cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(test_pca[0].cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(test_pca[0].cpu().numpy())\n",
    "plt.show()\n",
    "plt.hist(test_pca[0:1000].flatten().cpu().numpy(), bins=100)\n",
    "plt.show()\n",
    "# plt.plot(normalized_latent[0].cpu().numpy())\n",
    "# plt.show()\n",
    "# # plt.hist(normalized_latent[0:10].cpu().numpy(), bins=100)\n",
    "# # plt.show()\n",
    "# plt.plot(latent_mean.cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.plot(latent_std.cpu().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clip_embs = torch.load(\"/home/ubuntu/AutoLoRADiscovery/celeba_arc_embs.pt\", map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = []\n",
    "counter = 0\n",
    "for key in weight_dimensions.keys():\n",
    "    keyf = key.split(\".\")[2]+\".\"+key.split(\".\")[3]+\"/\"+key.split(\".\")[-4]+\"/\"+key.split(\".\")[-3]+\".\"+key.split(\".\")[-2]\n",
    "    main_group=key.split(\".\")[2]+\".\"+key.split(\".\")[3]\n",
    "    # if main_group not in segments:\n",
    "    #     segments[main_group] = [[], 0]\n",
    "    seg_std = weights[:, counter:counter+weight_dimensions[key][0][0]].std().item()\n",
    "    segments.append((keyf, counter, counter+weight_dimensions[key][0][0], weight_dimensions[key][0][0], seg_std))\n",
    "    # segments[main_group][1] += weight_dimensions[key][0][0]\n",
    "    # final_weights0[key] = flattened_weights[0, counter:counter+weight_dimensions[key][0][0]].unflatten(0, weight_dimensions[key][1])\n",
    "    counter += weight_dimensions[key][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_std_on_weights(weights):\n",
    "    weight_parts = []\n",
    "    for (key, start, end, length, std) in segments:\n",
    "        weight_parts.append(weights[:, start:end] / std)\n",
    "    return torch.cat(weight_parts, dim=-1)\n",
    "\n",
    "def deapply_std_on_weights(weights):\n",
    "    weight_parts = []\n",
    "    for (key, start, end, length, std) in segments:\n",
    "        weight_parts.append(weights[:, start:end] * std)\n",
    "    return torch.cat(weight_parts, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embs[\"000005.jpg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "new_map = {}\n",
    "new_weights = []\n",
    "with torch.no_grad():\n",
    "    for i, index in tqdm.tqdm(enumerate(idens.index.values), total=len(idens.index.values)):\n",
    "        if index not in clip_embs:\n",
    "            print (\"skipping\", index)\n",
    "            continue\n",
    "        ws = test_pca[i]#((weights[i] - mean) / std) @ V\n",
    "        new_map[index] = (\n",
    "            ws,\n",
    "            clip_embs[index],#.half()\n",
    "            idens.iloc[i].values,\n",
    "            i\n",
    "        )\n",
    "        # new_weights.append(ws)\n",
    "# new_weights = torch.stack(new_weights)\n",
    "# save new map into /mnt/rd/celeba_map.pt\n",
    "# torch.save(new_map, \"/mnt/rd/celeba_map.pt\")\n",
    "\n",
    "# idens\n",
    "# type(ids)\n",
    "# len(ids.index.values)\n",
    "# ids.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_map, \"/mnt/rd/celeba_vae_map_clip_arc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2w_to_lora(pca_coeffs):\n",
    "    return ((pca_coeffs) @ V.T) * std + mean\n",
    "recover_weight = w2w_to_lora(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reconstructed,\"/mnt/rd/reconstructed_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pca.min(), test_pca.max(), test_weight.min(), test_weight.max(), recover_weight.min(), recover_weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "device = \"cuda:0\"\n",
    "if os.path.exists(\"/mnt/rd/inference_lora\"):\n",
    "    os.system(\"rm -rf /mnt/rd/inference_lora\")\n",
    "\n",
    "unflatten(((reconstructed[0])).unsqueeze(0).detach().clone(), weight_dimensions, \"/mnt/rd/inference_lora\")\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v51\", \n",
    "                                        torch_dtype=torch.float16,safety_checker = None,\n",
    "                                        requires_safety_checker = False).to(device)\n",
    "\n",
    "\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, \"/mnt/rd/inference_lora/unet\", adapter_name=\"identity1\")\n",
    "adapters_weights1 = load_peft_weights(\"/mnt/rd/inference_lora/unet\", device=\"cuda:0\")\n",
    "pipe.unet.load_state_dict(adapters_weights1, strict = False)\n",
    "pipe.unet.to(\"cuda\", torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "images = pipe([\"A photo of a sks person\"] * 4, height=640, width=640, num_inference_steps=50, guidance_scale=3.0).images\n",
    "canvas = Image.new('RGB', (640*4, 640))\n",
    "for i, image in enumerate(images):\n",
    "    canvas.paste(image, (640*i, 0))\n",
    "\n",
    "imgbyte = io.BytesIO()\n",
    "canvas.save(imgbyte, format=\"JPEG\", quality=20)\n",
    "imgbyte.seek(0)\n",
    "display(IPImage(data=imgbyte.read(), width=640*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "new_map = {}\n",
    "new_weights = []\n",
    "with torch.no_grad():\n",
    "    for i, index in tqdm.tqdm(enumerate(idens.index.values), total=len(idens.index.values)):\n",
    "        ws = ((weights[i] - mean) / std) @ V\n",
    "        new_map[index] = (\n",
    "            ws,\n",
    "            clip_embs[index]\n",
    "        )\n",
    "        new_weights.append(ws)\n",
    "new_weights = torch.stack(new_weights)\n",
    "# save new map into /mnt/rd/celeba_map.pt\n",
    "# torch.save(new_map, \"/mnt/rd/celeba_map.pt\")\n",
    "\n",
    "# idens\n",
    "# type(ids)\n",
    "# len(ids.index.values)\n",
    "# ids.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_map[list(new_map.keys())[0]][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights.mean(), new_weights.std(), new_weights.shape, new_weights.min(), new_weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = new_weights.mean()\n",
    "centered_weights = new_weights - mean\n",
    "\n",
    "# 2. Calculate the absolute maximum for symmetric scaling\n",
    "max_abs_value = max(centered_weights.abs().max(), centered_weights.abs().min())\n",
    "\n",
    "# 3. Scale the data to [-1, 1] range\n",
    "scaled_weights = centered_weights / max_abs_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "170_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_weights.mean(), scaled_weights.std(), scaled_weights.shape, scaled_weights.min(), scaled_weights.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max([torch.max(new_map[i][0]) for i in new_map])\n",
    "min_value = min([torch.min(new_map[i][0]) for i in new_map])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_map, \"/mnt/rd/celeba_map.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_map[list(new_map.keys())[10]][0].min(), new_map[list(new_map.keys())[10]][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = torch.load(\"/mnt/rd/celeba_map.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_lora, key = ldd[list(ldd.keys())[0]][0], list(ldd.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(\"/mnt/rd/all_weights.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/AutoLoRADiscovery/discover_lora_diffusion/weights2weights/\")\n",
    "from discover_lora_diffusion.weights2weights.lora_w2w import LoRAw2w\n",
    "from discover_lora_diffusion.weights2weights.utils import unflatten\n",
    "from diffusers import DiffusionPipeline \n",
    "from peft import PeftModel\n",
    "from peft.utils.save_and_load import load_peft_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dimensions = torch.load(\"/mnt/rd/weight_dimensions.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unflatten(tst_lora.unsqueeze(0).clone().to(\"cuda\"), weight_dimensions, \"./example3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v51\", \n",
    "                                         torch_dtype=torch.float16,safety_checker = None,\n",
    "                                         requires_safety_checker = False).to(\"cuda\")\n",
    "   \n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, \"./example3/unet\", adapter_name=\"identity1\")\n",
    "adapters_weights1 = load_peft_weights(\"./example3/unet\", device=\"cuda:0\")\n",
    "pipe.unet.load_state_dict(adapters_weights1, strict = False)\n",
    "pipe.to(\"cuda\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#random seed generator\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "generator = generator.manual_seed(6)\n",
    "latents = torch.randn(\n",
    "        (1, pipe.unet.in_channels, 512 // 8, 512 // 8),\n",
    "        generator = generator,\n",
    "        device = \"cuda\"\n",
    "    ).half()\n",
    "\n",
    "#inference parameters\n",
    "prompt = \"A photo of a sks person\"\n",
    "negative_prompt = \"low quality, blurry, unfinished\"\n",
    "guidance_scale = 3.0\n",
    "ddim_steps = 50\n",
    "#run inference\n",
    "image = pipe([prompt]*2, num_inference_steps=ddim_steps, guidance_scale=guidance_scale, height=640, width=640).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"_class_name\": \"UnCLIPScheduler\",\n",
    "    \"_diffusers_version\": \"0.17.0.dev0\",\n",
    "    \"clip_sample\": True,\n",
    "    \"clip_sample_range\": 4.0,  # Adjusted to match your data range\n",
    "    \"num_train_timesteps\": 100,  # Reduced from 1000\n",
    "    \"prediction_type\": \"sample\",\n",
    "    \"variance_type\": \"fixed_small_log\"\n",
    "}\n",
    "\n",
    "scheduler = diffusers.UnCLIPScheduler.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "import warnings\n",
    "image_transforms = transforms.Compose([transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                                                transforms.RandomCrop(512),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize([0.5], [0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"/home/ubuntu/AutoLoRADiscovery/junk/1691527680903.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_input = image_transforms(img).unsqueeze(0).to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pipe.vae.encode(vae_input).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_c = latents *0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_c.min(), latents_c.max(), latents_c.mean(), latents_c.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.min(), latents.max(), latents.mean(), latents.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "scheduler = diffusers.UnCLIPScheduler.from_config(\"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\"_class_name\": \"UnCLIPScheduler\",\n",
    "\"_diffusers_version\": \"0.17.0.dev0\",\n",
    "\"clip_sample\": True,\n",
    "\"clip_sample_range\": 10.0,  # Adjusted to match your data range\n",
    "\"num_train_timesteps\": 100,  # Reduced from 1000\n",
    "\"prediction_type\": \"epsilon\",\n",
    "\"variance_type\": \"fixed_small_log\"\n",
    "}\n",
    "\n",
    "custom_scheduler = diffusers.UnCLIPScheduler.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheuler = diffusers.DDPMScheduler(num_train_timesteps=100, beta_start=0.0004, beta_end=0.999, beta_schedule=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheuler.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheuler.betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image as IPImage, display\n",
    "img_np = np.array(img)\n",
    "\n",
    "def show_img_lq(img):\n",
    "    img = Image.fromarray(img)\n",
    "    import io\n",
    "    buff_img = io.BytesIO()\n",
    "    img = img.resize((img.width // 5, img.height // 5))\n",
    "    img.save(buff_img, format=\"JPEG\", quality=20) \n",
    "    buff_img.seek(0)\n",
    "    display(IPImage(data=buff_img.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def show_noised_seq(img, scheduler):\n",
    "    with torch.no_grad():\n",
    "        img_list = []\n",
    "        img_tensor = image_transforms(img).unsqueeze(0).to(\"cuda\") \n",
    "        std_img = img_tensor.std()\n",
    "        mean_img = img_tensor.mean()\n",
    "        \n",
    "        img_tensor = (img_tensor - mean_img) / std_img\n",
    "        \n",
    "        for t in tqdm.trange(scheduler.num_train_timesteps):\n",
    "            noise = torch.randn_like(img_tensor) \n",
    "            img_noisy = scheduler.add_noise(img_tensor, noise, torch.Tensor([t]).long())\n",
    "            img_list.append((img_noisy, noise, t))\n",
    "        _img_list = [img_list[0]]\n",
    "        stp_v =(scheduler.num_train_timesteps//30)\n",
    "        for i in range(0, scheduler.num_train_timesteps, stp_v):\n",
    "            _img_list.append(img_list[i: i+stp_v][-1])\n",
    "        # _img_list.append(img_list[-1])\n",
    "\n",
    "        def show_imglistt(_img_list):\n",
    "            merged = torch.cat(_img_list, dim=3)\n",
    "            lv = _img_list[-1][0]\n",
    "            fv = _img_list[0][0]\n",
    "            print (lv.min(), lv.max(), lv.mean(), lv.std())\n",
    "            print (fv.min(), fv.max(), fv.mean(), fv.std())\n",
    "            print(merged.shape, merged.min(), merged.max(), merged.mean(), merged.std())\n",
    "            merged = (merged * std_img) + mean_img\n",
    "            merged = merged.cpu().numpy().squeeze() \n",
    "            merged = (merged + 1) / 2\n",
    "            merged = (merged * 255).astype(np.uint8)\n",
    "            merged = np.transpose(merged, (1,2,0))\n",
    "            show_img_lq(merged)\n",
    "        \n",
    "        show_imglistt([img_item[0] for img_item in _img_list])\n",
    "\n",
    "        # reversed\n",
    "        _reverse_img_list = []\n",
    "        for i in range(len(_img_list)-1, -1, -1):\n",
    "            (noised, noise, t) = _img_list[i]\n",
    "            print(t)\n",
    "            # noised = scheduler.scale_model_input(noised, t)\n",
    "            denoised = scheduler.step(noise, t, noised, return_dict=False)[0]\n",
    "            _reverse_img_list.append(denoised)\n",
    "        show_imglistt(_reverse_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.scheduler.set_timesteps(100)\n",
    "show_noised_seq(img, custom_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = torch.randn(1, 3, 512, 512)\n",
    "print(xl.shape, xl.min(), xl.max(), xl.mean(), xl.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
